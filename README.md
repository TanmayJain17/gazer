# ğŸ¯ Gazer: Real-Time Webcam-Based Eye Tracking

[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Flask](https://img.shields.io/badge/Flask-3.0+-green.svg)](https://flask.palletsprojects.com/)
[![MediaPipe](https://img.shields.io/badge/MediaPipe-0.10.3-orange.svg)](https://mediapipe.dev/)

A cost-effective, accessible eye-tracking solution using standard webcams and machine learning for real-time gaze estimation. Achieves **~89% accuracy** without requiring specialized hardware.

<p align="center">
  <img src="docs/images/demo.gif" alt="Gazer Demo" width="600">
</p>

> ğŸ“„ **Research Paper**: This project is based on our IEEE ICCCNT 2024 publication:  
> *"Real-Time Webcam-Based Eye Tracking for Gaze Estimation: Applications and Innovations"*

---

## âœ¨ Features

- **No Special Hardware** - Works with any standard webcam
- **Real-Time Tracking** - Low-latency gaze prediction using WebSockets
- **Cross-Platform** - Runs on desktop and mobile browsers
- **High Accuracy** - ~89% accuracy with proper calibration
- **Head Movement Compensation** - Combines pupil and head tracking
- **Blink Detection** - Filters out invalid data during blinks

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 â”‚     â”‚                  â”‚     â”‚                 â”‚
â”‚  Webcam Feed    â”‚â”€â”€â”€â”€â–¶â”‚  MediaPipe       â”‚â”€â”€â”€â”€â–¶â”‚  Flask Server   â”‚
â”‚  (Browser)      â”‚     â”‚  Face Landmarker â”‚     â”‚  (Python)       â”‚
â”‚                 â”‚     â”‚                  â”‚     â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                          â”‚
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     ML Pipeline                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ Pupil SVR   â”‚â”€â”€â”€â–¶â”‚  Stacking   â”‚â”€â”€â”€â–¶â”‚  Weighted Average   â”‚    â”‚
â”‚  â”‚ Base Model  â”‚    â”‚  (Linear)   â”‚    â”‚ (1.5Ã—pupil+0.5Ã—head)â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚               â”‚
â”‚  â”‚  Head SVR   â”‚â”€â”€â”€â–¶â”‚  Stacking   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚  â”‚ Base Model  â”‚    â”‚  (Linear)   â”‚                               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  Gaze Point     â”‚
              â”‚  (x, y) coords  â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“¸ Screenshots

<table>
  <tr>
    <td align="center">
      <img src="docs/images/calibration.png" width="300" alt="Calibration Screen"/>
      <br><em>21-Point Calibration</em>
    </td>
    <td align="center">
      <img src="docs/images/iris_detection.png" width="300" alt="Iris Detection"/>
      <br><em>MediaPipe Iris Detection</em>
    </td>
  </tr>
  <tr>
    <td align="center">
      <img src="docs/images/mobile_demo.png" width="300" alt="Mobile Demo"/>
      <br><em>Cross-Device Support</em>
    </td>
    <td align="center">
      <img src="docs/images/accuracy_plot.png" width="300" alt="Accuracy"/>
      <br><em>Gaze Prediction Results</em>
    </td>
  </tr>
</table>

---

## ğŸš€ Quick Start

### Prerequisites

- Python 3.9 or higher
- Webcam
- Modern browser (Chrome, Firefox, Safari)

### Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/gazer.git
cd gazer

# Create virtual environment
python -m venv venv

# Activate it
source venv/bin/activate  # macOS/Linux
# OR
venv\Scripts\activate     # Windows

# Install dependencies
pip install -r requirements.txt

# Run the application
python run.py
```

### Usage

1. Open your browser to **http://localhost:3226**
2. Click **"Start Eye Tracking"**
3. Allow camera access when prompted
4. Click **"Enable Webcam"** and position your face
5. Click **"Start Calibration"** and follow the dots with your eyes
6. After calibration, your gaze will be tracked in real-time!

---

## ğŸ“ Project Structure

```
gazer/
â”œâ”€â”€ app.py                  
â”œâ”€â”€ run.py                  
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”œâ”€â”€ LICENSE
â”œâ”€â”€ .gitignore
â”œâ”€â”€ static/
â”‚   â”œâ”€â”€ css/
â”‚   â”‚   â”œâ”€â”€ eyetracking.css
â”‚   â”‚   â””â”€â”€ style.css
â”‚   â””â”€â”€ js/
â”‚       â””â”€â”€ eyetracking.js  
â”œâ”€â”€ templates/
â”‚   â”œâ”€â”€ eyetracking.html    
â”‚   â””â”€â”€ instructions.html   
â””â”€â”€ docs/
    â””â”€â”€ images/
```

---

## ğŸ”¬ How It Works

### 1. Pupil Detection
Uses [MediaPipe Face Landmarker](https://developers.google.com/mediapipe/solutions/vision/face_landmarker) to detect 478 facial landmarks, extracting iris positions (landmarks 469-478) to calculate pupil centers.

### 2. Calibration
User follows 21 calibration points across the screen while the system collects:
- Left/right pupil coordinates
- Nose position (head tracking)
- Centroid of facial features

### 3. Model Training
Two-stage stacking ensemble:
1. **Base Models**: MultiOutput SVR with polynomial kernel for pupil and head data
2. **Stacking Models**: Linear regression to refine predictions
3. **Weighted Combination**: `final = 1.5 Ã— pupil_pred + 0.5 Ã— head_pred`

### 4. Validation
Optional validation phase trains a correction model to reduce systematic errors.

### 5. Real-Time Tracking
Predictions are smoothed using position history averaging to reduce jitter.

---

## ğŸ“Š Performance

| Metric | Value |
|--------|-------|
| Average Accuracy | ~89% |
| Calibration Points | 21 |
| Data Points per Calibration | 100 |
| Supported Browsers | Chrome, Firefox, Safari, Edge |
| Min Accuracy Threshold | 65% |

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

---

## ğŸ“š Citation

If you use this project in your research, please cite:

```bibtex
@inproceedings{jain2024realtime,
  title={Real-Time Webcam-Based Eye Tracking for Gaze Estimation: Applications and Innovations},
  author={Jain, Tanmay and Jain, Priyanka and Bhatia, Samiksha and Jain, N. K. and Sarkar, Chandan},
  booktitle={2024 15th International Conference on Computing Communication and Networking Technologies (ICCCNT)},
  year={2024},
  organization={IEEE},
  doi={10.1109/ICCCNT61001.2024.10724037}
}
```

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- [MediaPipe](https://mediapipe.dev/) for face landmark detection
- [Flask-SocketIO](https://flask-socketio.readthedocs.io/) for real-time communication
- [scikit-learn](https://scikit-learn.org/) for ML models
- Centre for Development of Advanced Computing (C-DAC), Delhi

---

## ğŸ“¬ Contact

**Tanmay Jain** - tanmay.jain260@gmail.com

Project Link: [https://github.com/tanmayjain17/gazer](https://github.com/tanmayjain17/gazer)

---

<p align="center">
  Made with â¤ï¸ for accessible eye-tracking technology
</p>
